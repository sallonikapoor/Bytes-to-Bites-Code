# -*- coding: utf-8 -*-
"""Working Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RGfTaDFJoukBPRnBrb1BQkSGFRXKEWXY
"""

import pandas as pd
import PIL as pil
from PIL import Image
import numpy as np
import os
from google.colab import drive

import matplotlib.pyplot as plt

#drive.mount('/content/drive/')


#file_path = '/content/drive/My Drive/Salloni/Data/From Nature Paper/train_val_test_dataset.csv'

actual_name = "train_val_test_dataset.csv"

codes_file = "first_level_admin_area_codes.csv"

combo_file = "with_admin_train_val_test.csv"

# Load the dataset
#file_path = '/mnt/data/train_val_test_dataset.csv'
data = pd.read_csv(actual_name)
codes = pd.read_csv(codes_file)

combo_df = pd.read_csv(combo_file)

# Determine which columns have all data
complete_columns = {column: data[column].isnull().sum()for column in data.columns}
complete_columns


temp_data_fcs_in = data[data['fcs'].notna()]

data_fcs_in = df = pd.DataFrame(data=temp_data_fcs_in)

# 1[A,B,C] [prevA, prevB, prevC] [prev2A, prev2B, prev2C]
# 2[A,B,C]
# 3[A,B,C] [prevA, prevB, prevC] [prev2A, prev2B, prev2C]
# 4[A,B,C] [prevA, prevB, prevC] [prev2A, prev2B, prev2C]
# 5[A,B,C] [prevA, prevB, prevC] [prev2A, prev2B, prev2C]
# 6[A,B,C] [prevA, prevB, prevC] [prev2A, prev2B, prev2C]

country_name = [""] * len(data_fcs_in)
region_name = [""] * len(data_fcs_in)

row_number = []
for k in range(len(data_fcs_in)):
  row_number.append(k)

print(row_number)
data_fcs_in['country_name'] = country_name
data_fcs_in['region_name'] = region_name
data_fcs_in['row_index'] = region_name

country_name = []
region_name = []
row_index = []
def code_lookup(thing_we_are_looking_up):
  for i in range(len(codes['adm1_code'])):
    found_it = 0
    if codes['adm1_code'][i] == thing_we_are_looking_up:
      found_it = 1
      return(str(codes['adm0_name'][i]), str(codes['adm1_name'][i]))
  if found_it == 0:
    return("No match", "No match")
for i in range(0): #list(data_fcs_in.index.values):
  row_index.append(i)
  try:
    this, that = code_lookup(int(data_fcs_in['adm1_code'][i]))
  except:
    print(i)
  country_name.append(this)
  region_name.append(that)
country_name = combo_df['country_name']
region_name = combo_df['region_name']
row_index = combo_df['row_index']

data_fcs_in['country_name'] = country_name
data_fcs_in['region_name'] = region_name
data_fcs_in['row_index'] = row_index

data_fcs_in.to_csv('with_admin_train_val_test.csv')

a = [1,2,2,2,3,4,5,5,6,2,3]

print(set(a))

countries_total = (set(data_fcs_in['country_name']))
real_total_country = countries_total

"num_fatalities_battles_remote_violence_90days_difference",
"single_pewi_mean_last_3months",
"num_fatalities_all_90days_difference",
"pop_density",
"num_fatalities_battles_vac_90days_difference",
"single_pewi_min_last_3months",
"num_fatalities_vac_remote_violence_90days_difference",
"num_fatalities_battles_90days_difference",
"num_fatalities_remote_violence_90days_difference",
"num_fatalities_vac_90days_difference",
"single_pewi_max_last_3months",
"headline_inflation_value",
"ce_variation_3months",
"food_inflation_value",
"last_fcs",
"last_rcsi",
"rainfall_3_months_anomaly_min_last_3month",
"rainfall_1_month_anomaly_min_last_3month",
"rainfall_1_month_anomaly_max_last_3month",
"ndvi_anomaly_max_last_3month",
"rainfall_value_min_last_3month",
"ndvi_anomaly_mean_last_3month",
"ndvi_value_min_last_3month",
"rainfall_1_month_anomaly_mean_last_3month",
"rainfall_value_mean_last_12month",
"rainfall_value_mean_last_3month",
"ndvi_value_max_last_3month",
"rainfall_value_max_last_3month",
"ndvi_value_mean_last_3month",
"ndvi_anomaly_min_last_3month",
"rainfall_3_months_anomaly_mean_last_3month",
"rainfall_3_months_anomaly_max_last_3month",
"ndvi_value_mean_last_12month"

a = 2
#print(len(clean_df))


# columns_needed = ["gdp_adm0","prevalence_of_undernourishment", "ndvi_value_mean_last_12month", "rainfall_3_months_anomaly_min_last_3month", "pop_density", "num_fatalities_battles_vac_90days_difference"]
columns_needed = [
"num_fatalities_battles_remote_violence_90days_difference",
"single_pewi_mean_last_3months",
"num_fatalities_all_90days_difference",
"pop_density",
"num_fatalities_battles_vac_90days_difference",
"single_pewi_min_last_3months",
"num_fatalities_vac_remote_violence_90days_difference",
"num_fatalities_battles_90days_difference",
"num_fatalities_remote_violence_90days_difference",
"num_fatalities_vac_90days_difference",
"single_pewi_max_last_3months",
"headline_inflation_value",
"ce_variation_3months",
"food_inflation_value",
"rainfall_3_months_anomaly_min_last_3month",
"rainfall_1_month_anomaly_min_last_3month",
"rainfall_1_month_anomaly_max_last_3month",
"ndvi_anomaly_max_last_3month",
"rainfall_value_min_last_3month",
"ndvi_anomaly_mean_last_3month",
"ndvi_value_min_last_3month",
"rainfall_1_month_anomaly_mean_last_3month",
"rainfall_value_mean_last_12month",
"rainfall_value_mean_last_3month",
"ndvi_value_max_last_3month",
"rainfall_value_max_last_3month",
"ndvi_value_mean_last_3month",
"ndvi_anomaly_min_last_3month",
"rainfall_3_months_anomaly_mean_last_3month",
"rainfall_3_months_anomaly_max_last_3month",
"ndvi_value_mean_last_12month"]

clean_df = data_fcs_in.dropna(subset=columns_needed)
# clean_country_df = clean_df
clean_country_df = clean_df.loc[clean_df['country_name'] == "Uganda"]

y = clean_country_df['fcs']

# columns_needed_to_train = ["gdp_adm0","prevalence_of_undernourishment", "ndvi_value_mean_last_12month", "rainfall_value_mean_last_12month", "pop_density", "num_fatalities_battles_vac_90days_difference"]
# for country use this:

X = clean_country_df[
    columns_needed
]

#for all data use this:
#X = clean_df[
#    columns_needed
#]
#
#

X.head()

NaN_after_cleaning = clean_df[columns_needed].isnull().sum()
print(NaN_after_cleaning)

X.head()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

#@title Instructor Solution
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)



regression_model = LinearRegression()

regression_model.fit(X_train, y_train)

predictions = regression_model.predict(X_test)

from sklearn.metrics import mean_absolute_error

mean_absolute_error(y_test, predictions)

regression_model.coef_

print(max(y_test))

import matplotlib.pyplot as plt


from sklearn.neural_network import MLPRegressor

nn_regressor = MLPRegressor(hidden_layer_sizes=(100, 80, 45))

nn_regressor.fit(X_train, y_train)

from sklearn.ensemble import RandomForestRegressor
forest = RandomForestRegressor(max_depth=50, random_state=0)
forest.fit(X_train, y_train)

predictions = forest.predict(X_test)

mean_absolute_error(y_test, predictions)


print(y_test)
print(predictions)

import matplotlib.pyplot as plt

# Plotting actual vs. predicted FCS scores
plt.figure(figsize=(8, 6))
plt.scatter(y_test, predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Diagonal line
plt.xlabel('Actual FCS')
plt.ylabel('Predicted FCS')
plt.title('Actual vs. Predicted FCS Scores')
plt.show()

print(mean_absolute_error(y_test, predictions))

print((X_test.values.flatten()))

forest.decision_path(X_test)

# Get numerical feature importances
importances = forest.feature_importances_

# Plot feature importances
features = X_train.columns
indices = np.argsort(importances)

plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

print(importances)
print(X_train.columns)

def order_features_by_values(values, features):
    # Combine the values and features into a list of tuples
    combined = list(zip(values, features))

    # Sort the combined list based on the values
    sorted_combined = sorted(combined, reverse=True)

    # Extract the sorted features from the sorted combined list
    sorted_features = [feature for value, feature in sorted_combined]
    return sorted_features

X_test_mini = []
for i in X_test[1:10].values:
  X_test_mini.append(i)

explainer = shap.KernelExplainer(model=nn_regressor.predict, data=X_train, link="identity")
shap_values = explainer.shap_values(X_test[1:10], nsamples=100)

shap.initjs()

print(shap_values)
shap.force_plot(explainer.expected_value, shap_values[1], X_test_mini[1])

nn_regressor.predict(X)

print(set(countries_total))

a = [2,3,4,1]
a2 = [2,4,1,5,6,1,4,2,4]
b = ["Algeria"]
c = []

for i in a2:
  try:
    c.append(a.index(i))
  except:
    c.append("null")

print(c)

top_5_list = []
table = []
newout = []

for country in real_total_country:
  print(str(country))
  # columns_needed = ["gdp_adm0","prevalence_of_undernourishment", "ndvi_value_mean_last_12month", "rainfall_3_months_anomaly_min_last_3month", "pop_density", "num_fatalities_battles_vac_90days_difference"]
  columns_total_relevant = [
  "num_fatalities_battles_remote_violence_90days_difference",
  "single_pewi_mean_last_3months",
  "num_fatalities_all_90days_difference",
  "pop_density",
  "num_fatalities_battles_vac_90days_difference",
  "single_pewi_min_last_3months",
  "num_fatalities_vac_remote_violence_90days_difference",
  "num_fatalities_battles_90days_difference",
  "num_fatalities_remote_violence_90days_difference",
  "num_fatalities_vac_90days_difference",
  "single_pewi_max_last_3months",
  "headline_inflation_value",
  "ce_variation_3months",
  "food_inflation_value",
  "rainfall_3_months_anomaly_min_last_3month",
  "rainfall_1_month_anomaly_min_last_3month",
  "rainfall_1_month_anomaly_max_last_3month",
  "ndvi_anomaly_max_last_3month",
  "rainfall_value_min_last_3month",
  "ndvi_anomaly_mean_last_3month",
  "ndvi_value_min_last_3month",
  "rainfall_1_month_anomaly_mean_last_3month",
  "rainfall_value_mean_last_12month",
  "rainfall_value_mean_last_3month",
  "ndvi_value_max_last_3month",
  "rainfall_value_max_last_3month",
  "ndvi_value_mean_last_3month",
  "ndvi_anomaly_min_last_3month",
  "rainfall_3_months_anomaly_mean_last_3month",
  "rainfall_3_months_anomaly_max_last_3month",
  "ndvi_value_mean_last_12month"]

  #ex_clean_df = data_fcs_in.dropna(subset=columns_total_relevant)
  # clean_country_df = clean_df

  #ex_clean_country_df = ex_clean_df.loc[ex_clean_df['country_name'] == country]
  #print("old data size was: " + str(len(ex_clean_country_df)) + " for " + str(country))

  initial =  data_fcs_in.loc[data_fcs_in['country_name'] == country]
  all_columns_in = list(initial.columns)
  column_count = initial.count()
  counter = 0
  columns_needed = []
  for i in all_columns_in:
      if column_count[i] > (len(initial) / 2 ) and (i in columns_total_relevant):
        columns_needed.append(i)
      counter = counter + 1



  clean_df = data_fcs_in.dropna(subset=columns_needed)
  # clean_country_df = clean_df

  clean_country_df = clean_df.loc[clean_df['country_name'] == country]

  #print("new data size is: " + str(len(clean_country_df)) + " for " + str(country))

  y = clean_country_df['fcs']

  # columns_needed_to_train = ["gdp_adm0","prevalence_of_undernourishment", "ndvi_value_mean_last_12month", "rainfall_value_mean_last_12month", "pop_density", "num_fatalities_battles_vac_90days_difference"]
  # for country use this:

  X = clean_country_df[
      columns_needed
  ]

  #for all data use this:
  #X = clean_df[
  #    columns_needed
  #]
  #
  #
  NaN_after_cleaning = clean_df[columns_needed].isnull().sum()
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LinearRegression

  if len(X) > 5:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    from sklearn.ensemble import RandomForestRegressor
    forest = RandomForestRegressor(max_depth=50, random_state=0)
    forest.fit(X_train, y_train)

    predictions = forest.predict(X_test)

    error_out =  mean_absolute_error(y_test, predictions)
    importances = forest.feature_importances_

  # Plot feature importances
    features = X_train.columns


    total_order = order_features_by_values(importances, X_train.columns)
    print(str(country) + " at " + str(mean_absolute_error(y_test, predictions) * 100) + "% error, top 5 is " + str(total_order[:5]))
    ordering = []
    for i in columns_total_relevant:
      try:
        ordering.append(total_order.index(i) + 1)
      except:
        ordering.append("null")
    table.append([country] + ordering)
    newout.append(error_out)

# convert array into dataframe

DF = pd.DataFrame(table)
df2 = pd.DataFrame(newout)
# save the dataframe as a csv file
df2.to_csv("newout.csv")

print(top_5_list)

list(columns_total_relevant).index('rainfall_1_month_anomaly_max_last_3month')

a = [1,2,3,4,5]

b = a[:3]
print(b)

count_list = [0] * len(columns_total_relevant)

for i in top_5_list:
  for j in i[1][:1]:
    location = columns_total_relevant.index(j)
    count_list[location] = count_list[location] + 1

print(count_list)

tick = 0
combo = []
for i in count_list:
  combo.append([columns_total_relevant[tick], i])
  tick = tick + 1

print(len(combo))

print(combo)